{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import visualkeras\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import MaxPooling2D, Conv2D, Input, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "ia.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(val_out, val_block_size, img_size, max_num, image_path):\n",
    "    def preprocess(img):\n",
    "        img = (img*255.0).astype(np.uint8)\n",
    "        img = np.squeeze(img, axis=0)\n",
    "        return img\n",
    "\n",
    "    preprocesed = preprocess(val_out)\n",
    "    # print(preprocesed.shape)\n",
    "    final_image = np.array([])\n",
    "    single_row = np.array([])\n",
    "    if max_num > val_out.shape[-1]:\n",
    "        num = val_out.shape[-1]\n",
    "    else:\n",
    "        num = max_num\n",
    "    if val_block_size > num:\n",
    "        print(\"val_block_size必須小於num\")\n",
    "        return\n",
    "    for b in range(num):\n",
    "        # concat image into a row\n",
    "        if single_row.size == 0:\n",
    "            single_row = preprocesed[ :, :, b]\n",
    "            single_row = cv2.resize(single_row, (img_size, img_size))\n",
    "        else:\n",
    "            single_row_now = preprocesed[:, :, b]\n",
    "            single_row_now = cv2.resize(single_row_now, (img_size, img_size))\n",
    "            single_row = np.concatenate((single_row, single_row_now), axis=1)\n",
    "\n",
    "        # concat image row to final_image\n",
    "        if (b+1) % val_block_size == 0:\n",
    "            if final_image.size == 0:\n",
    "                final_image = single_row\n",
    "            else:\n",
    "                final_image = np.concatenate((final_image, single_row), axis=0)\n",
    "\n",
    "            # reset single row\n",
    "            single_row = np.array([])\n",
    "    final_image = cv2.cvtColor(final_image, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(image_path, final_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-4\n",
    "    if epoch > 80:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 40:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 20:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 10:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN():\n",
    "    inputs = Input(shape=(32, 32, 3))\n",
    "    conv1 = Conv2D(filters=32, strides=(1, 1), kernel_size=3, input_shape=(32, 32, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(filters=32, strides=(1, 1), kernel_size=3, activation='relu', padding='same')(conv1)\n",
    "    MaxPool2D1 = MaxPooling2D(pool_size=2)(conv1)\n",
    "\n",
    "    conv2 = Conv2D(filters=64, strides=(1, 1), kernel_size=3, activation='relu', padding='same')(MaxPool2D1)\n",
    "    conv2 = Conv2D(filters=64, strides=(1, 1), kernel_size=3, activation='relu', padding='same')(conv2)\n",
    "    MaxPool2D2 = MaxPooling2D(pool_size=2)(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(filters=128, strides=(1, 1), kernel_size=3, activation='relu', padding='same')(MaxPool2D2)\n",
    "    conv3 = Conv2D(filters=128, strides=(1, 1), kernel_size=3, activation='relu', padding='same')(conv3)\n",
    "    MaxPool2D3 = MaxPooling2D(pool_size=2)(conv3)\n",
    "\n",
    "    \n",
    "    conv4 = Conv2D(filters=256, kernel_size=3, activation='relu', padding='same')(MaxPool2D3)\n",
    "    conv4 = Conv2D(filters=256, kernel_size=3, activation='relu', padding='same')(conv4)\n",
    "    MaxPool2D4 = MaxPooling2D(pool_size=2)(conv4)\n",
    "    \n",
    "    '''conv5 = Conv2D(filters=256, kernel_size=3, activation='relu', padding='same')(MaxPool2D4)\n",
    "    conv5 = Conv2D(filters=256, kernel_size=3, activation='relu', padding='same')(conv5)\n",
    "    MaxPool2D5 = MaxPooling2D(pool_size=2)(conv5)'''\n",
    "    \n",
    "\n",
    "    faltten1 = Flatten()(MaxPool2D4)\n",
    "\n",
    "    dense1 = Dense(512, activation='relu')(faltten1)\n",
    "    dropout4 = Dropout(rate=0.35)(dense1)\n",
    "    out = Dense(10, activation='softmax')(dropout4)     # softmax is important !!\n",
    "\n",
    "    model = Model(inputs, out, name = \"cnn\")\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=lr_scheduler(0)), # 0.0001\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "rand_aug = iaa.SomeOf((0, 2), [\n",
    "    iaa.Crop(px=(1, 16), keep_size=True),\n",
    "    iaa.Fliplr(0.5),\n",
    "    iaa.GaussianBlur(sigma=(0, 3.0))\n",
    "])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_aug = iaa.RandAugment(n=7, m=3)\n",
    "images_aug = rand_aug(images=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train\\'s shape  =   ', X_train.shape)\n",
    "print('images_aug\\'s shape = ', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_aug.show_grid([images_aug[0], images_aug[1]], cols=8, rows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "xx = 5\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original\")\n",
    "plt.imshow(np.array(X_train[xx]))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"SimpleAug\")\n",
    "plt.imshow(np.array(images_aug[xx]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug = np.concatenate([X_train, np.array(images_aug)])\n",
    "Y_train_aug = np.concatenate([Y_train, Y_train])\n",
    "\n",
    "x_train_aug = X_train_aug.astype('float32')/255.0\n",
    "x_test = X_test.astype('float32')/255.0\n",
    "\n",
    "# One-hot encoding\n",
    "y_train_aug = utils.to_categorical(Y_train_aug)\n",
    "y_test = utils.to_categorical(Y_test)\n",
    "\n",
    "print(x_train_aug.shape, y_train_aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "for kfold, (train, valid) in enumerate(KFold(n_splits=6, shuffle=True).split(x_train_aug, y_train_aug)):\n",
    "    # clear the session \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # calling the model and compile it \n",
    "    model = CNN()\n",
    "\n",
    "    print('------------------------------------------')\n",
    "    print('            Fold ' + str(kfold+1) + ' processing')\n",
    "    print('------------------------------------------')\n",
    "    \n",
    "    # run the model \n",
    "    history = model.fit(x_train_aug[train], y_train_aug[train], \n",
    "                        batch_size=32, \n",
    "                        epochs=100, \n",
    "                        validation_data=(x_train_aug[valid], y_train_aug[valid]), \n",
    "                        callbacks=[checkpoint, LearningRateScheduler(lr_scheduler)],\n",
    "                        shuffle=True, \n",
    "                        verbose=1)\n",
    "    # seq_model.save_weights(f'wg_{kfold}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_train_history(train_history, title, train, validation):\n",
    "    plt.plot(train_history.history[train])\n",
    "    plt.plot(train_history.history[validation])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('train')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'validation'], loc='center right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_train_history(history, 'Train History', 'accuracy', 'val_accuracy')\n",
    "show_train_history(history, 'Loss History', 'loss', 'val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualkeras.layered_view(model, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print('Test:')\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save process img\n",
    "'''cv2.imwrite('test.jpg', X_train[10])\n",
    "s = cv2.imread('test.jpg')\n",
    "img = cv2.resize(s, (32, 32))\n",
    "img_batch = np.expand_dims(img, axis=0)\n",
    "   \n",
    "for layer_name in ['conv2d', 'conv2d_1', 'max_pooling2d', 'conv2d_2', 'conv2d_3', 'max_pooling2d_1', 'conv2d_4', 'conv2d_5', 'max_pooling2d_2']:\n",
    "    conv1_layer = layer_model(model, layer_name)\n",
    "    conv_img = conv1_layer.predict(img_batch)\n",
    "    save_result(conv_img, 12, 32, 64, './' + 'Layer_img_' + layer_name + '.jpg')'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
